# About C. Morgan Davis

I'm a **data professional with 16+ years of hands-on engineering experience** across aerospace, defense, automotive, data center, medical, and consumer industries. My career began in mechanical engineering — but data was always at the center of what I did best: building cost models, running statistical analyses, automating workflows, and turning raw technical information into decisions that moved the needle. Over time, that thread became the whole story.

I hold a **B.S. in Mechanical Engineering** from the **University of Maryland, Baltimore County**.
- I'm proud to be a member of the **Tau Beta Pi Engineering Honor Society**.

I'm currently pursuing a **Master of Data Analytics (Big Data Systems Option)** at **Penn State World Campus**, with an expected graduation of May 2028. 

My Penn State coursework so far includes:
- Data Mining - DAAN 545
- Applied Statistics - STAT 500

I hold an **Active DoD Secret Clearance**.

---

## Where It Started

My career began with a year as a Mechanical Engineering Intern at a U.S. Marine Corps acquisition program office in Quantico, Virginia, where I reviewed contractor bids, supported validation testing, and designed mechanical components for programs like the FRC, MTVR-Trailer, Mobile Trauma Bay, and PUMA. It was early exposure to large, complex, multi-stakeholder technical programs — the kind where getting the analysis right actually matters.

From there I moved to **TriMech** in Richmond, Virginia (2010–2013), where I worked as an Applications Support Engineer providing technical support, training, and applications engineering for SolidWorks and Stratasys 3D printer customers across the East Coast. This role sharpened my ability to understand technical systems quickly and communicate them clearly to engineers at all levels — a skill that has served me in every data role since.

## Building the Analytical Foundation

At **Carpenter Co.** (2013–2015), the world's largest producer of comfort cushioning products, I transitioned into a Research Engineer role focused on polyurethane foam product development for the consumer bedding industry. Clients included Serta-Simmons, Tempur-Sealy, Walmart, and Costco. The work was deeply quantitative: I developed product performance claims using statistics-based methods, designed fabrication tooling, and learned firsthand how to translate experimental data into defensible product specifications.

This was where I started thinking seriously about data as a discipline — not just a byproduct of engineering work.

## Data Center Infrastructure and Patented Engineering

From 2015 to 2018, I worked as a **Mechanical Engineer III at Power Distribution, Inc.** (PDI) in Richmond, Virginia — a leading supplier of power distribution solutions for data centers, later acquired by Eaton Corporation. I engineered PDUs, remote power panels, static transfer switches, and tap-off boxes, and I administered the company's full suite of engineering software including SolidWorks, SolidWorks PDM, AutoCAD, Altium, and Arena.

Two of my designs from this period resulted in **U.S. patents**:
- **US 9,768,591** — PDU with Front Side Access to Transformer Compartment
- **US 9,979,166** — Electrical Cabinet Service Door with Integrated Deadfront

Beyond the hardware, I designed the company's engineering change management workflows in SolidWorks PDM and maintained BOM and work order control in Made2Manage ERP — early, practical experience with data governance and engineering data pipelines.

## Applied Analytics and the Cost Model That Changed Everything

At **Rack Processing Company** (2019–2021) in Dayton, Ohio, I served as Senior Design Engineer and Sales Engineer for a company that designs and manufactures custom electroplating racks for the automotive industry. I managed accounts in the automotive and consumer plumbing sectors that generated over **$1.5M in annual revenue** in 2019.

The most impactful work I did there was analytical: I built a rack product **cost estimation model in Excel and Python** using multiple statistical regression techniques, mapping quantifiable product features directly to cost outputs. The result was a reduction in cost estimate turnaround time from approximately 48 hours to about 10 minutes — roughly a **99% reduction**. This project crystallized for me what data work at its best looks like: a well-scoped model, clean inputs, and an output that people actually use.

## Engineering Software Administration as a Data Problem

At **Aerobiotix** (2021–2023) in Miamisburg, Ohio, I worked as a Development Engineer on medical-grade air handling technologies, including the AeroCure VAC+, AeroCure Plume, and Illuvia Sense product lines. These systems use UVC light and composite-media filtration to remove biological, chemical, and particulate contaminants from intake air.

Alongside the hardware engineering, I designed, implemented, and maintained the company's **SolidWorks PDM SQL vault** and its associated workflows and macros. Managing a PDM system is, at its core, a data management problem — schema design, access control, workflow logic, and the integrity of every engineering record the company produces. I also conducted formal root cause analyses (Fault Trees, FMEA, Fishbone, 5 Why) during product development, bringing structured analytical thinking to the quality process.

## Defense Research and Computational Pipelines

From 2023 to 2025, I worked as a **CAD Engineer for Computational Physical Sciences at Applied Research Associates (ARA)** in Centerville, Ohio — an employee-owned research and engineering firm with deep roots in defense technologies. My main client was the Air Force Research Laboratory (AFRL) at Wright Patterson Air Force Base.

My primary work involved building **data and simulation pipelines** for electromagnetic modeling of aerospace and defense assets. I wrote CUBIT scripts to generate simulation-ready mesh files from CAD geometry, developed Python APIs to input boundary conditions and excitations into simulation tools (CREATE-RF SENTRi, Xpatch, Altair FEKO), and wrote Batch, Bash, PBS, and Slurm scripts to execute those simulations across distributed high-performance computing clusters. I also developed RF signature CAD models using photogrammetry techniques in SolidWorks.

This role was the clearest expression yet of what I've been building toward: writing code that moves data through complex pipelines to produce analytical outputs, in a domain where accuracy is non-negotiable.

## What I'm Doing Now

I'm currently a **Graduate Student at Penn State University** (January 2026–present), pursuing a Master of Data Analytics with a Big Data Systems concentration. 

As of January 2026, I'm leading a group project for DAAN 545 (Data Mining) that applies EDA, pattern mining (Apriori and FPGrowth), clustering, PCA, and supervised learning to the [InstaCart Online Grocery Basket Analysis Dataset](https://www.kaggle.com/datasets/yasserh/instacart-online-grocery-basket-analysis-dataset) — 33M+ purchase line items across 200K+ users — to reveal patterns in customer purchase behavior and inform recommendation systems.

On the personal project side, I'm actively building and shipping software:

**[OneTaskAtATime](https://github.com/cmdavis25/OneTaskAtATime)** — A focused, priority-smart desktop to-do manager built in Python, SQL, and PyQt5. The app presents one task at a time — the highest-importance actionable item — and resolves priority ties using an Elo ranking algorithm. It tracks deferral reasons for trend analysis, resurfaces stalled tasks via Windows Toast notifications, and supports task dependencies and subtask decomposition.

**[Ganttoro](https://github.com/cmdavis25/Ganttoro)** — A Monte Carlo-driven Gantt chart tool built in Python with PySide6, NumPy, SciPy, NetworkX, and Matplotlib. Rather than presenting a single optimistic timeline, Ganttoro runs thousands of schedule simulations using probabilistic task durations (triangular, PERT, or normal distributions) and propagates uncertainty through dependency chains. The result is a box plot timeline showing P10, P50, and P90 completion dates — because "3–7 days" is more honest than "5 days."

My technical toolkit spans SQL, Python (Pandas, matplotlib, scikit-learn), R (ggplot2), Power BI, Tableau, DAX, Git/GitHub, and a range of AI/LLM tools I use regularly. My statistics background covers hypothesis testing, multiple regression, DOE, Monte Carlo simulation, PCA, and NLP. 

I hold certifications in IBM Data Science, Power BI, Tableau, and DAX/Time Intelligence.

---

## The Through-Line

What ties all of this together isn't any single industry or job title — **it's a consistent pattern of using data to solve hard, real-world problems in environments where precision matters.** I've built models that cut 48-hour processes down to 10 minutes. I've designed simulation pipelines that run on distributed HPC clusters. I've administered data systems that engineering teams depend on every day.

I bring a rare combination: the rigor of an engineer trained to get things right, the breadth of someone who has worked in multiple industries, and the technical fluency of someone who has been working with data and building models for most of their career.

**If you're looking for a data professional who can operate at the intersection of analysis, engineering, and delivery —**

**Let's talk.**